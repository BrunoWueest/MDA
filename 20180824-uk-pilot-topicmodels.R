##### Media-driven accountability
# UK-PROTOTYPE
# bw and ms

# Do not clear your workspace, otherwise the working directory changes.
# Restart R if you have to free memory or detach packages, objects or commands
# rm(list = ls(envir = globalenv()), envir = globalenv())

# do not set the working directory every time but use pathes relative to your git clone--which
# should be located in the Dropbox of this project (the one called 'chapters')
setwd("/home/pdm2admin/model")
getwd()

### PREPARATION ###
#------------------

#load corpus
corpus <- readRDS("../data/UKCorpus_PeakLow.rds") # note the relative path. This should now work for everyone.

#load libraries
library(tm)
library(stm)
library(lubridate)

#first, we have to prepare our texts for the analysis

# topic models should not be run across languages (we should discuss machine translating everything!)

#take only peak time articles
corpus <- corpus[order(corpus$article_date, decreasing = T),]
corpus$weeks <- week(corpus$article_date)
corpus <- corpus[corpus$weeks != 53,]
corpus$years <- year(corpus$article_date)
corpus$week <- as.numeric(as.factor(as.Date(paste(corpus$years,
                     corpus$weeks, 1, sep="-"), "%Y-%U-%u")))

# preprocess texts (rather rough but quite standard). Denny and Spirling
# (https://www.nyu.edu/projects/spirling/documents/preprocessing.pdf) argue that we should evaluate this, too (?).
texts <- Corpus(VectorSource(corpus$article_text))
texts <- tm_map(texts, tolower)
texts <- tm_map(texts, removePunctuation)
texts <- tm_map(texts, removeNumbers)
texts <- tm_map(texts, stripWhitespace)
for (i in 1:length(corpus$article_text)) {
  corpus$text_preprocessed[i] <- gsub("\\s", " ", texts[[i]])
}
corpus$text_preprocessed <- gsub("\\s+", " ", corpus$text_preprocessed)
corpus$text_preprocessed <- gsub("^\\s+|\\s+$", "", corpus$text_preprocessed)

# recode some vars. The stm likes factors - m.n. in the postestimation -, so we encode our indicators accordingly
corpus$actor_type[corpus$actor_type %in% c("hybrid", "private")] <- "hybrid_private"
corpus$actor_type <- as.factor(corpus$actor_type)
corpus$tonality_verbalized <- as.factor(corpus$tonality_verbalized)
corpus$policy_scope <- as.factor(corpus$policy_scope)
corpus$territorial_scope[corpus$territorial_scope %in% c("subnational", "regional")] <- "regional_national"
corpus$territorial_scope <- as.factor(corpus$territorial_scope)
corpus$media_source <- as.factor(corpus$media_source)
corpus$entity_name <- as.factor(corpus$entity_name)
#is not not easier to work with entity_id numbers?
corpus$media_type[corpus$media_type %in% c("regional", "tabloid_or_free")] <- "regional_tabloid_free"
#I did the same grouping in my phd and it works well
corpus$media_type[corpus$media_type %in% c("magazines", "quality")] <- "quality_magazines"
#corpus$media_type[corpus$media_type %in% c("magazines", "quality")] <- "quality_and_magazines" #for clarity
corpus$media_type <- as.factor(corpus$media_type)
corpus$peak <- as.factor(corpus$peak)
corpus$policy_output[corpus$policy_output %in% c("hard", "hard / soft")] <- "hard_soft"
#In IP2 it is likely that all hard is concentrated in the EC, again with a separate analysis in the chapter this should not be a problem.
corpus$policy_output <- as.factor(corpus$policy_output)
corpus$policy_field_1 <- tolower(corpus$policy_field_1)
corpus$policy_field_1 <- ifelse(corpus$policy_field_1 %in% c("multiple", "not definable", "", "."), "multiple", "specific")
corpus$policy_field_1[is.na(corpus$policy_field_1)] <- "multiple"
corpus$policy_field_1 <- as.factor(corpus$policy_field_1)
corpus <- corpus[!is.na(corpus$policy_output),]


#write out corpus for the w2v evaluation (see below)
write.csv(corpus$text_preprocessed, "../data/corpus_w2v.csv", row.names = F, fileEncoding = "utf-8")

# Generate a corpus for the stm 
corpus_stm <- textProcessor(corpus[,c("text_preprocessed")], metadata=corpus, stem=T,
                        language="english", removestopwords=T, lowercase=F,
                        removenumbers=F, removepunctuation=F, customstopwords = c("will", "said", "can", "also", "may"))
corpus_stm <- prepDocuments(corpus_stm$documents, corpus_stm$vocab, corpus_stm$meta, lower.thresh = 10)

saveRDS(corpus_stm, "../data/corpus.rds")

# Since we will use a deterministic (spectral) initialization of our topic model, many paramters of the topic model will 
# be fixed. We only need an evaluation of the optimal number of topics, which we achive in three steps

## 1. run a topic model for every granularity in the range of interest

# define range of topic numbers for evaluation
ks <- 3:15 #for thesting, we only evaluate 4 models, a more meaningful range is 3:50 or even 3:100

# define the prevalence formula (basically a regression of indicators on the topic prevalences generated by the model)
# for clarity
formula_prev <- "prevalence =~ tonality_verbalized + territorial_scope + entity_name + peak + policy_output + policy_field_1 + actor_type + policy_scope + media_type + media_source + media_country + article_word_count + s(week) + functional_scope_informative + functional_scope_implementing + functional_scope_decisive"

# evaluate on the 100 most probable words per topic
n_words <- 100
rm(texts, i, corpus)

# loop over ks and write out most probable words per topic for each model
for (k in ks) {
 STM <- stm(corpus_stm$documents, corpus_stm$vocab, K=30, 
            as.formula(formula_prev), data = corpus_stm$meta,
            init.type="Spectral", verbose=T, 
            control = list(nits = 100, burnin = 25))
 words <- labelTopics(STM, n = n_words)
 words <- as.data.frame(words[[1]])
 outname <- paste0("./results/words_", k, "_", ".txt")
 write.table(words, outname, col.names = F, row.names = F, sep = "\t", quote = F)
} 
#this command seems to take too much working space and cannot be conducted; what to do? > ask Lucien Baumgartner (luciengeorge.baumgartner@uzh.ch) for
# a spot to my old production server. Lucien can give you a Login to an RStudio-instance that will be able to stem this large files.
# BTW: My three years old MacBook Air can run the models.

## 2. build a w2v model. I will not go into the details, but w2v is a word embedding model
#     that allows to estimate a semantic space for each word (i.e. space of dimensions of related words)
#     w2v is implemented in gensim in python, which is why we run an external script here
command = "C:/Users/noerber/AppData/Local/Programs/Python/Python37" # type "which python" in your terminal to determine the path to your python dist
path2script = "train_w2v_model.py"
corpus_path = "corpus_w2v.csv"
model_path = "w2v_model.txt"

allArgs = c(path2script, "english", corpus_path, model_path)# Build up args in a vector
system2(command, args=allArgs) #run the python script

# 3. Now we calculate coherence (how similar are words within a topic) and discrimination (how
#    dissimilar are word of a topic compared to the words of all other topics)
#    for more, see: https://www.fabriziogilardi.org/resources/papers/policy-diffusion-issue-definition.pdf

#run w2v evaluation
command = "C:/Users/noerber/AppData/Local/Programs/Python/Python37"
path2script = "coherence.py"
model_path = "w2v_model.txt"
allArgs = c(path2script, model_path)
system2(command, args=allArgs)

# let's see which k is best...
w2v <- read.csv("stm-evaluation.txt", header = F)
colnames(w2v) <- c("coherence", "coherence.discrimination", "k")
w2v[w2v$coherence.discrimination == max(w2v$coherence.discrimination),]
# ...the topic model with k = 6

#plot the evaluation (smoothed line not neat with only 4 models)
library(ggplot2)
plott <- ggplot(w2v, aes(x=k, y=coherence.discrimination)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method = "loess", colour = "black", span = 0.5)
ggsave(plot=plott, paste("./Results/Graphs/w2v_evaluation.pdf"), height=5, width=8.09)


## ok, let's run the model for the optimal number of topics
k <- 6
STM <- stm(corpus_stm$documents, corpus_stm$vocab, K=k, as.formula(formula_prev),
           data = corpus_stm$meta, init.type="Spectral", verbose = T,
           control = list(nits = 100, burnin = 25))# Fit stm

#save model output
saveRDS(STM, "../data/STM_3.rds")
STM <- readRDS("../data/STM_15.rds")

# Save 100 most probable words per topic
words <- labelTopics(STM, n = 100)
words <- as.data.frame(t(words[[1]]))
write.table(words, file = "./results/words_15.txt", col.names = T, row.names = F, sep = "\t", quote = F)

k <- 15

#save 5 most relevant documents per topic
thoughts <- findThoughts(STM, texts = corpus_stm$meta$article_text, n = 5, topics = 1:k)
thoughtsDocs <- data.frame(matrix(unlist(thoughts$docs), nrow=k, byrow=T))
thoughtsIndices <- data.frame(matrix(unlist(thoughts$index), nrow=k, byrow=T))
thoughts <- cbind(thoughtsDocs, thoughtsIndices)
write.csv(thoughts, paste0("./results/thoughts_", k, ".csv"),
          fileEncoding = "utf-8")

# Write out probability, exclusivity and frequency of each word per topic
require(matrixStats)

#A helper: a James-Stein Estimator Shrinking to a Uniform Distribution
#This draws from Hausser and Strimmer (2009) JMLR.
js.estimate <- function(prob, ct) {
  if(ct<=1) {
    #basically if we only observe a count of 1
    #the variance goes to infinity and we get the uniform distribution.
    return(rep(1/length(prob), length(prob)))
  }
  # MLE of prob estimate
  mlvar <- prob*(1-prob)/(ct-1)
  unif <- rep(1/length(prob), length(prob)) 
  
  # Deviation from uniform
  deviation <- sum((prob-unif)^2)
  
  #take care of special case,if no difference it doesn't matter
  if(deviation==0) return(prob)
  
  lambda <- sum(mlvar)/deviation
  #if despite  our best efforts we ended up with an NaN number-just return the uniform distribution.
  if(is.nan(lambda)) return(unif)
  
  #truncate
  if(lambda>1) lambda <- 1
  if(lambda<0) lambda <- 0
  
  #Construct shrinkage estimator as convex combination of the two
  lambda*unif + (1 - lambda)*prob
}
safelog <- function(x) {
  out <- log(x)
  out[which(out< -1000)] <- -1000
  out
}

logbeta <- STM$beta$logbeta[[1]]
topics <- 1:nrow(logbeta) 
K <- STM$settings$dim$K
wordcounts <- STM$settings$dim$wcounts$x
excl <- t(t(logbeta) - colLogSumExps(logbeta))
excl <- safelog(sapply(1:ncol(excl), function(x) js.estimate(exp(excl[,x]), wordcounts[x])))
freqscore <- apply(logbeta,1,rank)/ncol(logbeta)
freqscore <- as.data.frame(freqscore)
colnames(freqscore) <- paste("frequency_", 1:K, sep = "")
exclscore <- apply(excl,1,rank)/ncol(logbeta)
exclscore <- as.data.frame(exclscore)
colnames(exclscore) <- paste("exclusivity_", 1:K, sep = "")
beta <- lapply(STM$beta$logbeta, exp)
probscore <- as.data.frame(t(beta[[1]]))
colnames(probscore) <- paste("probability_", 1:K, sep = "")
scores <- cbind(probscore, exclscore, freqscore)
scores$vocab <- STM$vocab

outname <- paste0("./results/scores_15.txt")
write.table(scores, outname, col.names = T, row.names = F, sep = "\t", quote = F)


# produce word-plots

# Read data and check its structure
words <- read.delim2("./results/words_15.txt")
scores <- read.delim2("./results/scores_15.txt")
scores <- data.frame(scores)
is.data.frame(scores)
names(scores)
head(scores)
sapply(scores, mode)

# render relevant variables numeric
co <- ncol(scores) - 1
scores[, c(1:co)] <- sapply(scores[, c(1:co)], as.numeric)
head(scores)

# Identify the Columns associated with probabilities, exclusivities and frequencies
pr <- seq(1,co/3)
ex <- seq(max(pr) + 1, max(pr) + co/3)
fr <- seq(max(ex) + 1, max(ex) + co/3)

library(ggplot2)

# Plot and Save Top 50 (n) Words for each Topic (ncol(words)) by Prob, Excl and Freq
n <- 50 
for(i in 1:ncol(words)){
  topic <- data.frame(c(scores[, c(pr[i], ex[i], fr[i], co + 1)], i))
  colnames(topic) <- c("Probability", "Exclusivity", "Frequency", "Words", "Topic")
  topic <- topic[ topic[,4] %in% words[1:n,i] ,]
  pdf(file = paste("./results/wordplot_15_topic_", i, ".pdf", sep = ""), paper = "special", width=7, height=5.5)
  print(ggplot(data = topic, aes(x = Exclusivity, y = Frequency, label = Words)) + geom_text(aes(size = Probability)) + theme(legend.position = "none") + ggtitle(paste("Topic ", i, sep = "")))
  dev.off()
}

# correlations with covariates from the prevalence formula
# weeks
prep <- estimateEffect(1:k ~ s(week), STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "week")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "WeeksPassed")
outname <- paste0("./results/effects_15_", "WeeksPassed.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)

#estimate peak
prep <- estimateEffect(1:k ~ peak, STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "peak", model = STM, method="pointestimate")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "peak")
outname <- paste0("./results/effects_15_", "peak.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)

#estimate media_type
prep <- estimateEffect(1:k ~ media_type, STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "media_type", model = STM, method="pointestimate")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "media_type")
outname <- paste0("./results/effects_15_", "media_type.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)

#estimate actor_type
prep <- estimateEffect(1:k ~ actor_type, STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "actor_type", model = STM, method="pointestimate")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "media_type")
outname <- paste0("./results/effects_15_", "actor_type.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)

#estimate tonality_verbalized
prep <- estimateEffect(1:k ~ tonality_verbalized, STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "tonality_verbalized", model = STM, method="pointestimate")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "tonality_verbalized")
outname <- paste0("./results/effects_15_", "tonality_verbalized.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)

#estimate territorial_scope
prep <- estimateEffect(1:k ~ territorial_scope, STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "territorial_scope", model = STM, method="pointestimate")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "territorial_scope")
outname <- paste0("./results/effects_15_", "territorial_scope.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)

#estimate policy_output
prep <- estimateEffect(1:k ~ policy_output, STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "policy_output", model = STM, method="pointestimate")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "policy_output")
outname <- paste0("./results/effects_15_", "policy_output.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)

#estimate policy_scope
prep <- estimateEffect(1:k ~ policy_scope, STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "policy_scope", model = STM, method="pointestimate")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "policy_scope")
outname <- paste0("./results/effects_15_", "policy_scope.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)

#estimate functional_scope_informative
prep <- estimateEffect(1:k ~ functional_scope_informative, STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "functional_scope_informative", model = STM, method="pointestimate")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "functional_scope_informative")
outname <- paste0("./results/effects_15_", "functional_scope_informative.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)

#estimate functional_scope_implementing
prep <- estimateEffect(1:k ~ functional_scope_implementing, STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "functional_scope_implementing", model = STM, method="pointestimate")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "functional_scope_implementing")
outname <- paste0("./results/effects_15_", "functional_scope_implementing.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)

#estimate functional_scope_decisive
prep <- estimateEffect(1:k ~ functional_scope_decisive, STM, meta = corpus_stm$meta)
x <- plot(prep, covariate = "functional_scope_decisive", model = STM, method="pointestimate")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1,], x$cis[[i]][2,])
  df <- cbind(df, i, x$uvals)
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("pointEstimate", "lb", "ub", "topic", "functional_scope_decisive")
outname <- paste0("./results/effects_15_", "functional_scope_decisive.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)



#########



# difference actor type
x <- plot(prep, covariate = "actor.type", model = STM, method = "difference", cov.value1="public (non-elected)", 
          cov.value2="public (elected)")
dfs <- data.frame()
for (i in 1:length(x$means)){
  df <- data.frame(x$means[[i]], x$cis[[i]][1], x$cis[[i]][2])
  df <- cbind(df, i, x$labels[[i]])
  dfs <- rbind(dfs, df)
}
colnames(dfs) <- c("diff_nonelec_elec", "lb", "ub", "topic", "actor.type")
outname <- paste0("./Results/difference_", "ActorType_diff.txt")
write.table(dfs, outname, col.names = T, row.names = F, sep = "\t", quote = F)


#####################
# correlation plots 

# correlation of topic prevalence with daily trend
d <- data.frame(read.delim(paste0("./results/effects_15_", "WeeksPassed.txt")))

d <- d[order(d$topic, d$WeeksPassed),]
length(d$WeeksPassed)
max(d$WeeksPassed)
mo <- seq(as.Date("2005-01-01"), by = "week", length.out = max(d$WeeksPassed))

topic_labels <- 1:15 # can be used later to define substantial topics
d$topicLabels <- NA
d$date <- rep(mo, length(topic_labels))

colnames(d)[5] <- c("var")

d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))

pdf(file="./results/correlation_15_weekspassed.pdf", paper="special", width=12, height=9)
print(ggplot(data = d, aes(x = date, y = pointEstimate)) +
        geom_line() +
        geom_hline(aes(yintercept = pointEstimateAverage), linetype = 2) +
        facet_wrap(~ topicLabels, ncol = 3) +
        labs(x = "weeks", y = "Topic prevalence", title = "") +
        theme(legend.position = "bottom", legend.title = element_blank(), legend.key.size = unit(0.5, "cm")) +
        scale_alpha(guide = "none") +
        geom_ribbon(aes(ymin = lb, ymax= ub, alpha = 0.5))) 
dev.off()

# correlation of topic prevalence with peak
file_name <- "./results/effects_15_peak.txt"
var_label_long <- "?"

d <- data.frame(read.delim(file_name))
var_label <- colnames(d)[5]
colnames(d)[5] <- c("var")

d$topicLabels <- NA
d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))
d$var <- as.factor(d$var)

pdf(file=paste("./results/correlations_15_peak.pdf", sep=""), paper="special", width=7, height=9)
print(
  ggplot(data = d, aes(x = topicLabels, y = pointEstimate, ymin = lb, ymax = ub, shape = var)) + #color = groupLabels, 
    geom_pointrange(position=position_dodge(width=0.5)) +
    coord_flip() +
    geom_hline(yintercept = 1/k, lty = 2, size = 0.5) +
    labs(x = "", y = "Topic prevalence", title = "") +
    theme(legend.position = "bottom", legend.title = element_blank())
)
dev.off()

# correlation of topic prevalence with media_type
file_name <- "./results/effects_15_media_type.txt"
var_label_long <- "?"

d <- data.frame(read.delim(file_name))
var_label <- colnames(d)[5]
colnames(d)[5] <- c("var")

d$topicLabels <- NA
d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))
d$var <- as.factor(d$var)

pdf(file=paste("./results/correlations_15_media_type.pdf", sep=""), paper="special", width=7, height=9)
print(
  ggplot(data = d, aes(x = topicLabels, y = pointEstimate, ymin = lb, ymax = ub, shape = var)) + #color = groupLabels, 
    geom_pointrange(position=position_dodge(width=0.5)) +
    coord_flip() +
    geom_hline(yintercept = 1/k, lty = 2, size = 0.5) +
    labs(x = "", y = "Topic prevalence", title = "") +
    theme(legend.position = "bottom", legend.title = element_blank())
)
dev.off()


# correlation of topic prevalence with actor_type
file_name <- "./results/effects_15_actor_type.txt"
var_label_long <- "?"

d <- data.frame(read.delim(file_name))
var_label <- colnames(d)[5]
colnames(d)[5] <- c("var")

d$topicLabels <- NA
d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))
d$var <- as.factor(d$var)

pdf(file=paste("./results/correlations_15_actor_type.pdf", sep=""), paper="special", width=7, height=9)
print(
  ggplot(data = d, aes(x = topicLabels, y = pointEstimate, ymin = lb, ymax = ub, shape = var)) + #color = groupLabels, 
    geom_pointrange(position=position_dodge(width=0.5)) +
    coord_flip() +
    geom_hline(yintercept = 1/k, lty = 2, size = 0.5) +
    labs(x = "", y = "Topic prevalence", title = "") +
    theme(legend.position = "bottom", legend.title = element_blank())
)
dev.off()


# correlation of topic prevalence with tonality_verbalized
file_name <- "./results/effects_15_tonality_verbalized.txt"
var_label_long <- "?"

d <- data.frame(read.delim(file_name))
var_label <- colnames(d)[5]
colnames(d)[5] <- c("var")

d$topicLabels <- NA
d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))
d$var <- as.factor(d$var)

pdf(file=paste("./results/correlations_15_tonality_verbalized.pdf", sep=""), paper="special", width=7, height=9)
print(
  ggplot(data = d, aes(x = topicLabels, y = pointEstimate, ymin = lb, ymax = ub, shape = var)) + #color = groupLabels, 
    geom_pointrange(position=position_dodge(width=0.5)) +
    coord_flip() +
    geom_hline(yintercept = 1/k, lty = 2, size = 0.5) +
    labs(x = "", y = "Topic prevalence", title = "") +
    theme(legend.position = "bottom", legend.title = element_blank())
)
dev.off()



# correlation of topic prevalence with territorial_scope
file_name <- "./results/effects_15_territorial_scope.txt"
var_label_long <- "?"

d <- data.frame(read.delim(file_name))
var_label <- colnames(d)[5]
colnames(d)[5] <- c("var")

d$topicLabels <- NA
d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))
d$var <- as.factor(d$var)

pdf(file=paste("./results/correlations_15_territorial_scope.pdf", sep=""), paper="special", width=7, height=9)
print(
  ggplot(data = d, aes(x = topicLabels, y = pointEstimate, ymin = lb, ymax = ub, shape = var)) + #color = groupLabels, 
    geom_pointrange(position=position_dodge(width=0.5)) +
    coord_flip() +
    geom_hline(yintercept = 1/k, lty = 2, size = 0.5) +
    labs(x = "", y = "Topic prevalence", title = "") +
    theme(legend.position = "bottom", legend.title = element_blank())
)
dev.off()


# correlation of topic prevalence with policy_output
file_name <- "./results/effects_15_policy_output.txt"
var_label_long <- "?"

d <- data.frame(read.delim(file_name))
var_label <- colnames(d)[5]
colnames(d)[5] <- c("var")

d$topicLabels <- NA
d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))
d$var <- as.factor(d$var)

pdf(file=paste("./results/correlations_15_policy_output.pdf", sep=""), paper="special", width=7, height=9)
print(
  ggplot(data = d, aes(x = topicLabels, y = pointEstimate, ymin = lb, ymax = ub, shape = var)) + #color = groupLabels, 
    geom_pointrange(position=position_dodge(width=0.5)) +
    coord_flip() +
    geom_hline(yintercept = 1/k, lty = 2, size = 0.5) +
    labs(x = "", y = "Topic prevalence", title = "") +
    theme(legend.position = "bottom", legend.title = element_blank())
)
dev.off()


# correlation of topic prevalence with policy_scope
file_name <- "./results/effects_15_policy_scope.txt"
var_label_long <- "?"

d <- data.frame(read.delim(file_name))
var_label <- colnames(d)[5]
colnames(d)[5] <- c("var")

d$topicLabels <- NA
d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))
d$var <- as.factor(d$var)

pdf(file=paste("./results/correlations_15_policy_scope.pdf", sep=""), paper="special", width=7, height=9)
print(
  ggplot(data = d, aes(x = topicLabels, y = pointEstimate, ymin = lb, ymax = ub, shape = var)) + #color = groupLabels, 
    geom_pointrange(position=position_dodge(width=0.5)) +
    coord_flip() +
    geom_hline(yintercept = 1/k, lty = 2, size = 0.5) +
    labs(x = "", y = "Topic prevalence", title = "") +
    theme(legend.position = "bottom", legend.title = element_blank())
)
dev.off()


# correlation of topic prevalence with functional_scope_decisive
file_name <- "./results/effects_15_functional_scope_decisive.txt"
var_label_long <- "?"

d <- data.frame(read.delim(file_name))
var_label <- colnames(d)[5]
colnames(d)[5] <- c("var")

d$topicLabels <- NA
d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))
d$var <- as.factor(d$var)

pdf(file=paste("./results/correlations_15_functional_scope_decisive.pdf", sep=""), paper="special", width=7, height=9)
print(
  ggplot(data = d, aes(x = topicLabels, y = pointEstimate, ymin = lb, ymax = ub, shape = var)) + #color = groupLabels, 
    geom_pointrange(position=position_dodge(width=0.5)) +
    coord_flip() +
    geom_hline(yintercept = 1/k, lty = 2, size = 0.5) +
    labs(x = "", y = "Topic prevalence", title = "") +
    theme(legend.position = "bottom", legend.title = element_blank())
)
dev.off()


# correlation of topic prevalence with functional_scope_implementing
file_name <- "./results/effects_15_functional_scope_implementing.txt"
var_label_long <- "?"

d <- data.frame(read.delim(file_name))
var_label <- colnames(d)[5]
colnames(d)[5] <- c("var")

d$topicLabels <- NA
d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))
d$var <- as.factor(d$var)

pdf(file=paste("./results/correlations_15_functional_scope_implementing.pdf", sep=""), paper="special", width=7, height=9)
print(
  ggplot(data = d, aes(x = topicLabels, y = pointEstimate, ymin = lb, ymax = ub, shape = var)) + #color = groupLabels, 
    geom_pointrange(position=position_dodge(width=0.5)) +
    coord_flip() +
    geom_hline(yintercept = 1/k, lty = 2, size = 0.5) +
    labs(x = "", y = "Topic prevalence", title = "") +
    theme(legend.position = "bottom", legend.title = element_blank())
)
dev.off()

file_name <- "./results/effects_15_functional_scope_informative.txt"
var_label_long <- "?"

d <- data.frame(read.delim(file_name))
var_label <- colnames(d)[5]
colnames(d)[5] <- c("var")

d$topicLabels <- NA
d$pointEstimateAverage <- NA

for(j in 1:length(topic_labels)){
  d$topicLabels[d$topic == j] <- topic_labels[j]
  d$pointEstimateAverage[d$topic == j] <- mean(d$pointEstimate[d$topic == j])
}

d$topicLabels <- factor(d$topicLabels, levels = unique(d$topicLabels[order(d$pointEstimateAverage, decreasing = TRUE)]))
d$var <- as.factor(d$var)

pdf(file=paste("./results/correlations_15_functional_scope_informative.pdf", sep=""), paper="special", width=7, height=9)
print(
  ggplot(data = d, aes(x = topicLabels, y = pointEstimate, ymin = lb, ymax = ub, shape = var)) + #color = groupLabels, 
    geom_pointrange(position=position_dodge(width=0.5)) +
    coord_flip() +
    geom_hline(yintercept = 1/k, lty = 2, size = 0.5) +
    labs(x = "", y = "Topic prevalence", title = "") +
    theme(legend.position = "bottom", legend.title = element_blank())
)
dev.off()
